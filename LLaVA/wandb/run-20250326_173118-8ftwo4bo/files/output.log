  0%|                                                                   | 0/1563 [00:00<?, ?it/s]/gscratch/krishna/mahtab/miniconda3/envs/newllava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/gscratch/krishna/mahtab/miniconda3/envs/newllava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  1%|▋                                                       | 20/1563 [04:34<5:35:29, 13.05s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
{'loss': 10.8742, 'learning_rate': 4.255319148936171e-06, 'epoch': 0.0}
{'loss': 10.8883, 'learning_rate': 8.510638297872341e-06, 'epoch': 0.0}
{'loss': 10.8321, 'learning_rate': 1.2765957446808511e-05, 'epoch': 0.0}
{'loss': 10.4604, 'learning_rate': 1.7021276595744682e-05, 'epoch': 0.0}
{'loss': 10.0345, 'learning_rate': 2.1276595744680852e-05, 'epoch': 0.0}
{'loss': 9.3721, 'learning_rate': 2.5531914893617022e-05, 'epoch': 0.0}
{'loss': 8.8748, 'learning_rate': 2.9787234042553192e-05, 'epoch': 0.0}
{'loss': 8.2966, 'learning_rate': 3.4042553191489365e-05, 'epoch': 0.01}
{'loss': 7.8768, 'learning_rate': 3.829787234042553e-05, 'epoch': 0.01}
{'loss': 7.2027, 'learning_rate': 4.2553191489361704e-05, 'epoch': 0.01}
{'loss': 6.5442, 'learning_rate': 4.680851063829788e-05, 'epoch': 0.01}
{'loss': 5.7482, 'learning_rate': 5.1063829787234044e-05, 'epoch': 0.01}
{'loss': 5.41, 'learning_rate': 5.531914893617022e-05, 'epoch': 0.01}
{'loss': 4.9444, 'learning_rate': 5.9574468085106384e-05, 'epoch': 0.01}
{'loss': 4.8197, 'learning_rate': 6.382978723404256e-05, 'epoch': 0.01}
{'loss': 4.5114, 'learning_rate': 6.808510638297873e-05, 'epoch': 0.01}
{'loss': 4.3673, 'learning_rate': 7.23404255319149e-05, 'epoch': 0.01}
{'loss': 4.3128, 'learning_rate': 7.659574468085106e-05, 'epoch': 0.01}
{'loss': 4.29, 'learning_rate': 8.085106382978723e-05, 'epoch': 0.01}
{'loss': 4.1904, 'learning_rate': 8.510638297872341e-05, 'epoch': 0.01}
save models to ./checkpoints/train_depth_annealing-llava-v1.5-13b-task-lora-tessst/checkpoint-20
Non-default generation parameters: {'max_length': 4096}
/gscratch/krishna/mahtab/miniconda3/envs/newllava/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/gscratch/krishna/mahtab/miniconda3/envs/newllava/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
  2%|▉                                                       | 27/1563 [07:37<6:57:02, 16.29s/it]
{'loss': 4.1039, 'learning_rate': 8.936170212765958e-05, 'epoch': 0.01}
{'loss': 4.0926, 'learning_rate': 9.361702127659576e-05, 'epoch': 0.01}
{'loss': 4.0851, 'learning_rate': 9.787234042553192e-05, 'epoch': 0.01}
{'loss': 4.0397, 'learning_rate': 0.00010212765957446809, 'epoch': 0.02}
{'loss': 3.9201, 'learning_rate': 0.00010638297872340425, 'epoch': 0.02}
{'loss': 3.9213, 'learning_rate': 0.00011063829787234043, 'epoch': 0.02}
{'loss': 3.9626, 'learning_rate': 0.00011489361702127661, 'epoch': 0.02}
