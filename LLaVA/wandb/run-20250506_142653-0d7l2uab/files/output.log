  0%|                                                                 | 0/12500 [00:00<?, ?it/s]/gscratch/krishna/mahtab/miniconda3/envs/newllava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/gscratch/krishna/mahtab/miniconda3/envs/newllava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
tensor(54.4596, device='cuda:0', grad_fn=<AddBackward0>) tensor(4.3125, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
  0%|                                                     | 11/12500 [00:47<14:36:39,  4.21s/it]
{'loss': 54.4596, 'learning_rate': 5.333333333333333e-07, 'epoch': 0.0}
tensor(55.2460, device='cuda:0', grad_fn=<AddBackward0>) tensor(4.3750, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 55.246, 'learning_rate': 1.0666666666666667e-06, 'epoch': 0.0}
tensor(54.1892, device='cuda:0', grad_fn=<AddBackward0>) tensor(4.2812, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 54.1892, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.0}
tensor(54.6085, device='cuda:0', grad_fn=<AddBackward0>) tensor(4.3125, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 54.6085, 'learning_rate': 2.1333333333333334e-06, 'epoch': 0.0}
tensor(54.2564, device='cuda:0', grad_fn=<AddBackward0>) tensor(4.2812, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 54.2564, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.0}
tensor(54.2946, device='cuda:0', grad_fn=<AddBackward0>) tensor(4.2812, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 54.2946, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.0}
tensor(54.1118, device='cuda:0', grad_fn=<AddBackward0>) tensor(4.2812, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 54.1118, 'learning_rate': 3.7333333333333337e-06, 'epoch': 0.0}
tensor(53.1937, device='cuda:0', grad_fn=<AddBackward0>) tensor(4.1875, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 53.1937, 'learning_rate': 4.266666666666667e-06, 'epoch': 0.0}
tensor(51.4025, device='cuda:0', grad_fn=<AddBackward0>) tensor(4.0312, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 51.4025, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.0}
tensor(49.9410, device='cuda:0', grad_fn=<AddBackward0>) tensor(3.8906, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 49.941, 'learning_rate': 5.333333333333334e-06, 'epoch': 0.0}
tensor(49.1356, device='cuda:0', grad_fn=<AddBackward0>) tensor(3.8281, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 49.1356, 'learning_rate': 5.866666666666667e-06, 'epoch': 0.0}
tensor(45.6132, device='cuda:0', grad_fn=<AddBackward0>) tensor(3.5000, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 45.6132, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.0}
tensor(44.3047, device='cuda:0', grad_fn=<AddBackward0>) tensor(3.4062, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 44.3047, 'learning_rate': 6.933333333333334e-06, 'epoch': 0.0}
tensor(41.1697, device='cuda:0', grad_fn=<AddBackward0>) tensor(3.1406, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 41.1697, 'learning_rate': 7.4666666666666675e-06, 'epoch': 0.0}
tensor(40.0227, device='cuda:0', grad_fn=<AddBackward0>) tensor(3.0625, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 40.0227, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}
tensor(38.2020, device='cuda:0', grad_fn=<AddBackward0>) tensor(2.9219, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 38.202, 'learning_rate': 8.533333333333334e-06, 'epoch': 0.0}
tensor(36.0503, device='cuda:0', grad_fn=<AddBackward0>) tensor(2.7500, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 36.0503, 'learning_rate': 9.066666666666667e-06, 'epoch': 0.0}
tensor(34.1444, device='cuda:0', grad_fn=<AddBackward0>) tensor(2.5625, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 34.1444, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.0}
tensor(32.4627, device='cuda:0', grad_fn=<AddBackward0>) tensor(2.4062, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 32.4627, 'learning_rate': 1.0133333333333333e-05, 'epoch': 0.0}
tensor(30.4704, device='cuda:0', grad_fn=<AddBackward0>) tensor(2.1875, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 30.4704, 'learning_rate': 1.0666666666666667e-05, 'epoch': 0.0}
save models to ./checkpoints/noneedtrain_depth_annealing-llava-v1.5-7b-task-lora/checkpoint-20
Non-default generation parameters: {'max_length': 4096}
/gscratch/krishna/mahtab/miniconda3/envs/newllava/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/gscratch/krishna/mahtab/miniconda3/envs/newllava/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
tensor(28.1610, device='cuda:0', grad_fn=<AddBackward0>) tensor(1.9766, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
  0%|‚ñè                                                    | 31/12500 [02:14<14:38:47,  4.23s/it]
{'loss': 28.161, 'learning_rate': 1.1200000000000001e-05, 'epoch': 0.0}
tensor(26.5187, device='cuda:0', grad_fn=<AddBackward0>) tensor(1.8125, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 26.5187, 'learning_rate': 1.1733333333333333e-05, 'epoch': 0.0}
tensor(25.0200, device='cuda:0', grad_fn=<AddBackward0>) tensor(1.6797, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 25.02, 'learning_rate': 1.2266666666666667e-05, 'epoch': 0.0}
tensor(23.6885, device='cuda:0', grad_fn=<AddBackward0>) tensor(1.5469, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 23.6885, 'learning_rate': 1.2800000000000001e-05, 'epoch': 0.0}
tensor(22.2678, device='cuda:0', grad_fn=<AddBackward0>) tensor(1.4062, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 22.2678, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.0}
tensor(21.3024, device='cuda:0', grad_fn=<AddBackward0>) tensor(1.3047, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 21.3024, 'learning_rate': 1.3866666666666667e-05, 'epoch': 0.0}
tensor(19.6921, device='cuda:0', grad_fn=<AddBackward0>) tensor(1.1641, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 19.6921, 'learning_rate': 1.44e-05, 'epoch': 0.0}
tensor(18.2941, device='cuda:0', grad_fn=<AddBackward0>) tensor(1.0391, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 18.2941, 'learning_rate': 1.4933333333333335e-05, 'epoch': 0.0}
tensor(16.9226, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.9180, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 16.9226, 'learning_rate': 1.546666666666667e-05, 'epoch': 0.0}
tensor(15.8727, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.8203, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 15.8727, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.0}
tensor(14.6737, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.7227, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 14.6737, 'learning_rate': 1.6533333333333333e-05, 'epoch': 0.0}
tensor(13.8436, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.6523, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 13.8436, 'learning_rate': 1.7066666666666667e-05, 'epoch': 0.0}
tensor(12.9464, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5742, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 12.9464, 'learning_rate': 1.76e-05, 'epoch': 0.0}
tensor(12.1938, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5273, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
