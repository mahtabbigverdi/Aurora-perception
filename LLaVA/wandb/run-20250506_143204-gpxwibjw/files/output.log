  0%|                                                                 | 0/12500 [00:00<?, ?it/s]/gscratch/krishna/mahtab/miniconda3/envs/newllava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/gscratch/krishna/mahtab/miniconda3/envs/newllava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
tensor(61.3817, device='cuda:0', grad_fn=<AddBackward0>) 11.631693840026855 tensor(4.9688, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
  0%|                                                     | 11/12500 [00:48<14:36:46,  4.21s/it]
{'loss': 61.3817, 'learning_rate': 5.333333333333333e-07, 'epoch': 0.0}
tensor(62.1456, device='cuda:0', grad_fn=<AddBackward0>) 11.645641326904297 tensor(5.0625, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 62.1456, 'learning_rate': 1.0666666666666667e-06, 'epoch': 0.0}
tensor(61.0486, device='cuda:0', grad_fn=<AddBackward0>) 11.548640251159668 tensor(4.9375, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 61.0486, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.0}
tensor(61.0710, device='cuda:0', grad_fn=<AddBackward0>) 11.571049690246582 tensor(4.9375, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 61.071, 'learning_rate': 2.1333333333333334e-06, 'epoch': 0.0}
tensor(60.5944, device='cuda:0', grad_fn=<AddBackward0>) 11.594411849975586 tensor(4.9062, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 60.5944, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.0}
tensor(60.5328, device='cuda:0', grad_fn=<AddBackward0>) 11.532760620117188 tensor(4.9062, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 60.5328, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.0}
tensor(60.4218, device='cuda:0', grad_fn=<AddBackward0>) 11.421757698059082 tensor(4.9062, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 60.4218, 'learning_rate': 3.7333333333333337e-06, 'epoch': 0.0}
tensor(59.1171, device='cuda:0', grad_fn=<AddBackward0>) 11.367053985595703 tensor(4.7812, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 59.1171, 'learning_rate': 4.266666666666667e-06, 'epoch': 0.0}
tensor(57.5762, device='cuda:0', grad_fn=<AddBackward0>) 11.326227188110352 tensor(4.6250, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 57.5762, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.0}
tensor(55.2323, device='cuda:0', grad_fn=<AddBackward0>) 11.232301712036133 tensor(4.4062, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 55.2323, 'learning_rate': 5.333333333333334e-06, 'epoch': 0.0}
tensor(53.7861, device='cuda:0', grad_fn=<AddBackward0>) 11.036073684692383 tensor(4.2812, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
{'loss': 53.7861, 'learning_rate': 5.866666666666667e-06, 'epoch': 0.0}
tensor(49.7998, device='cuda:0', grad_fn=<AddBackward0>) 10.799819946289062 tensor(3.8906, device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<MseLossBackward0>)
