  0%|                                                                 | 0/12500 [00:00<?, ?it/s]/gscratch/krishna/mahtab/miniconda3/envs/newllava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/gscratch/krishna/mahtab/miniconda3/envs/newllava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
^^^^^ losses:  58.45005798339844 11.450057029724121 4.6875
  0%|                                                     | 16/12500 [01:09<14:34:53,  4.20s/it]
{'loss': 58.4501, 'learning_rate': 5.333333333333333e-07, 'epoch': 0.0}
^^^^^ losses:  58.577762603759766 11.327763557434082 4.71875
{'loss': 58.5778, 'learning_rate': 1.0666666666666667e-06, 'epoch': 0.0}
^^^^^ losses:  57.91231918334961 11.412318229675293 4.65625
{'loss': 57.9123, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.0}
^^^^^ losses:  58.66554260253906 11.41554069519043 4.71875
{'loss': 58.6655, 'learning_rate': 2.1333333333333334e-06, 'epoch': 0.0}
^^^^^ losses:  57.608890533447266 11.358891487121582 4.625
{'loss': 57.6089, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.0}
^^^^^ losses:  57.941627502441406 11.44162654876709 4.65625
{'loss': 57.9416, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.0}
^^^^^ losses:  57.51940155029297 11.269400596618652 4.625
{'loss': 57.5194, 'learning_rate': 3.7333333333333337e-06, 'epoch': 0.0}
^^^^^ losses:  56.406253814697266 11.156254768371582 4.53125
{'loss': 56.4063, 'learning_rate': 4.266666666666667e-06, 'epoch': 0.0}
^^^^^ losses:  53.96538543701172 11.215386390686035 4.28125
{'loss': 53.9654, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.0}
^^^^^ losses:  52.506710052490234 11.006710052490234 4.15625
{'loss': 52.5067, 'learning_rate': 5.333333333333334e-06, 'epoch': 0.0}
^^^^^ losses:  50.90520095825195 10.65520191192627 4.03125
{'loss': 50.9052, 'learning_rate': 5.866666666666667e-06, 'epoch': 0.0}
^^^^^ losses:  48.01139831542969 10.261398315429688 3.765625
{'loss': 48.0114, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.0}
^^^^^ losses:  45.26414489746094 9.764142990112305 3.546875
{'loss': 45.2641, 'learning_rate': 6.933333333333334e-06, 'epoch': 0.0}
^^^^^ losses:  41.73581314086914 9.23581314086914 3.25
{'loss': 41.7358, 'learning_rate': 7.4666666666666675e-06, 'epoch': 0.0}
^^^^^ losses:  40.04460906982422 8.919608116149902 3.109375
{'loss': 40.0446, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}
^^^^^ losses:  38.222145080566406 8.722143173217773 2.953125
{'loss': 38.2221, 'learning_rate': 8.533333333333334e-06, 'epoch': 0.0}
^^^^^ losses:  35.85529327392578 8.480295181274414 2.734375
{'loss': 35.8553, 'learning_rate': 9.066666666666667e-06, 'epoch': 0.0}
^^^^^ losses:  33.40083312988281 8.400834083557129 2.5
{'loss': 33.4008, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.0}
^^^^^ losses:  31.947547912597656 8.322547912597656 2.359375
{'loss': 31.9475, 'learning_rate': 1.0133333333333333e-05, 'epoch': 0.0}
^^^^^ losses:  29.439533233642578 8.314532279968262 2.109375
{'loss': 29.4395, 'learning_rate': 1.0666666666666667e-05, 'epoch': 0.0}
save models to ./checkpoints/noneedtrain_depth_annealing-llava-v1.5-7b-task-lora/checkpoint-20
Non-default generation parameters: {'max_length': 4096}
/gscratch/krishna/mahtab/miniconda3/envs/newllava/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/gscratch/krishna/mahtab/miniconda3/envs/newllava/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
^^^^^ losses:  27.37587547302246 8.250875473022461 1.9140625
  0%|▏                                                    | 36/12500 [02:35<14:35:34,  4.21s/it]
{'loss': 27.3759, 'learning_rate': 1.1200000000000001e-05, 'epoch': 0.0}
^^^^^ losses:  25.547855377197266 8.17285442352295 1.7421875
{'loss': 25.5479, 'learning_rate': 1.1733333333333333e-05, 'epoch': 0.0}
^^^^^ losses:  23.79303741455078 8.168037414550781 1.5625
{'loss': 23.793, 'learning_rate': 1.2266666666666667e-05, 'epoch': 0.0}
^^^^^ losses:  22.353961944580078 8.103962898254395 1.421875
{'loss': 22.354, 'learning_rate': 1.2800000000000001e-05, 'epoch': 0.0}
^^^^^ losses:  21.11423110961914 8.051732063293457 1.3046875
{'loss': 21.1142, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.0}
^^^^^ losses:  19.977672576904297 8.22767162322998 1.171875
{'loss': 19.9777, 'learning_rate': 1.3866666666666667e-05, 'epoch': 0.0}
^^^^^ losses:  18.385679244995117 8.135679244995117 1.0234375
{'loss': 18.3857, 'learning_rate': 1.44e-05, 'epoch': 0.0}
^^^^^ losses:  17.102924346923828 8.040424346923828 0.90625
{'loss': 17.1029, 'learning_rate': 1.4933333333333335e-05, 'epoch': 0.0}
^^^^^ losses:  15.831324577331543 7.831324577331543 0.80078125
{'loss': 15.8313, 'learning_rate': 1.546666666666667e-05, 'epoch': 0.0}
^^^^^ losses:  14.796805381774902 7.734305381774902 0.70703125
{'loss': 14.7968, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.0}
^^^^^ losses:  13.745423316955566 7.495423316955566 0.625
{'loss': 13.7454, 'learning_rate': 1.6533333333333333e-05, 'epoch': 0.0}
^^^^^ losses:  13.128419876098633 7.472170352935791 0.56640625
{'loss': 13.1284, 'learning_rate': 1.7066666666666667e-05, 'epoch': 0.0}
^^^^^ losses:  12.213873863220215 7.151373863220215 0.5078125
{'loss': 12.2139, 'learning_rate': 1.76e-05, 'epoch': 0.0}
^^^^^ losses:  11.752826690673828 7.00282621383667 0.4765625
{'loss': 11.7528, 'learning_rate': 1.8133333333333335e-05, 'epoch': 0.0}
^^^^^ losses:  11.338109970092773 6.806859493255615 0.453125
{'loss': 11.3381, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.0}
^^^^^ losses:  10.816408157348633 6.628908157348633 0.41796875
{'loss': 10.8164, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.0}
^^^^^ losses:  10.46882438659668 6.4688239097595215 0.400390625
{'loss': 10.4688, 'learning_rate': 1.9733333333333333e-05, 'epoch': 0.0}
^^^^^ losses:  10.030105590820312 6.092606067657471 0.39453125
{'loss': 10.0301, 'learning_rate': 2.0266666666666667e-05, 'epoch': 0.0}
^^^^^ losses:  9.530808448791504 5.858933448791504 0.3671875
{'loss': 9.5308, 'learning_rate': 2.08e-05, 'epoch': 0.0}
^^^^^ losses:  9.197190284729004 5.822190284729004 0.337890625
{'loss': 9.1972, 'learning_rate': 2.1333333333333335e-05, 'epoch': 0.0}
save models to ./checkpoints/noneedtrain_depth_annealing-llava-v1.5-7b-task-lora/checkpoint-40
Non-default generation parameters: {'max_length': 4096}
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
^^^^^ losses:  8.846184730529785 5.596184730529785 0.32421875
  0%|▏                                                    | 56/12500 [04:01<14:34:18,  4.22s/it]
{'loss': 8.8462, 'learning_rate': 2.186666666666667e-05, 'epoch': 0.0}
^^^^^ losses:  8.579745292663574 5.689120292663574 0.2890625
{'loss': 8.5797, 'learning_rate': 2.2400000000000002e-05, 'epoch': 0.0}
^^^^^ losses:  8.221053123474121 5.627303123474121 0.259765625
{'loss': 8.2211, 'learning_rate': 2.2933333333333333e-05, 'epoch': 0.0}
^^^^^ losses:  7.757771968841553 5.304646968841553 0.2451171875
{'loss': 7.7578, 'learning_rate': 2.3466666666666667e-05, 'epoch': 0.0}
^^^^^ losses:  7.534958839416504 5.331833839416504 0.2197265625
{'loss': 7.535, 'learning_rate': 2.4e-05, 'epoch': 0.0}
^^^^^ losses:  7.305935382843018 5.274685382843018 0.203125
{'loss': 7.3059, 'learning_rate': 2.4533333333333334e-05, 'epoch': 0.0}
^^^^^ losses:  6.925776958465576 5.035151958465576 0.189453125
{'loss': 6.9258, 'learning_rate': 2.5066666666666665e-05, 'epoch': 0.0}
^^^^^ losses:  6.798012733459473 5.008950233459473 0.1787109375
{'loss': 6.798, 'learning_rate': 2.5600000000000002e-05, 'epoch': 0.0}
^^^^^ losses:  6.586828708648682 4.914953708648682 0.1669921875
{'loss': 6.5868, 'learning_rate': 2.6133333333333333e-05, 'epoch': 0.0}
^^^^^ losses:  6.255870342254639 4.677745342254639 0.158203125
{'loss': 6.2559, 'learning_rate': 2.6666666666666667e-05, 'epoch': 0.0}
^^^^^ losses:  6.360960960388184 4.923460960388184 0.1435546875
{'loss': 6.361, 'learning_rate': 2.7200000000000004e-05, 'epoch': 0.0}
^^^^^ losses:  5.9734697341918945 4.6297197341918945 0.134765625
{'loss': 5.9735, 'learning_rate': 2.7733333333333334e-05, 'epoch': 0.0}
^^^^^ losses:  5.904539585113525 4.646727085113525 0.1259765625
{'loss': 5.9045, 'learning_rate': 2.8266666666666668e-05, 'epoch': 0.0}
^^^^^ losses:  5.800772666931152 4.628897666931152 0.1171875
{'loss': 5.8008, 'learning_rate': 2.88e-05, 'epoch': 0.0}
^^^^^ losses:  5.571767330169678 4.485829830169678 0.10888671875
{'loss': 5.5718, 'learning_rate': 2.9333333333333336e-05, 'epoch': 0.0}
^^^^^ losses:  5.466464042663574 4.443026542663574 0.1025390625
{'loss': 5.4665, 'learning_rate': 2.986666666666667e-05, 'epoch': 0.0}
^^^^^ losses:  5.5762104988098145 4.5918354988098145 0.0986328125
{'loss': 5.5762, 'learning_rate': 3.04e-05, 'epoch': 0.0}
^^^^^ losses:  5.444224834442139 4.577037334442139 0.0869140625
{'loss': 5.4442, 'learning_rate': 3.093333333333334e-05, 'epoch': 0.0}
^^^^^ losses:  5.101311683654785 4.238030433654785 0.08642578125
{'loss': 5.1013, 'learning_rate': 3.146666666666667e-05, 'epoch': 0.0}
^^^^^ losses:  5.0328240394592285 4.2437615394592285 0.0791015625
{'loss': 5.0328, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.0}
save models to ./checkpoints/noneedtrain_depth_annealing-llava-v1.5-7b-task-lora/checkpoint-60
Non-default generation parameters: {'max_length': 4096}
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
^^^^^ losses:  5.082020282745361 4.343739032745361 0.07373046875
  1%|▎                                                    | 76/12500 [05:28<14:31:20,  4.21s/it]
{'loss': 5.082, 'learning_rate': 3.253333333333333e-05, 'epoch': 0.0}
^^^^^ losses:  5.078192710876465 4.394598960876465 0.068359375
{'loss': 5.0782, 'learning_rate': 3.3066666666666666e-05, 'epoch': 0.0}
^^^^^ losses:  4.740864276885986 4.072895526885986 0.06689453125
{'loss': 4.7409, 'learning_rate': 3.3600000000000004e-05, 'epoch': 0.01}
^^^^^ losses:  5.0084733963012695 4.4147233963012695 0.0595703125
{'loss': 5.0085, 'learning_rate': 3.4133333333333334e-05, 'epoch': 0.01}
^^^^^ losses:  4.751430034637451 4.185023784637451 0.056640625
{'loss': 4.7514, 'learning_rate': 3.466666666666667e-05, 'epoch': 0.01}
^^^^^ losses:  4.843221187591553 4.269002437591553 0.057373046875
{'loss': 4.8432, 'learning_rate': 3.52e-05, 'epoch': 0.01}
^^^^^ losses:  4.751633644104004 4.224289894104004 0.052734375
{'loss': 4.7516, 'learning_rate': 3.573333333333333e-05, 'epoch': 0.01}
^^^^^ losses:  4.590001583099365 4.078282833099365 0.05126953125
{'loss': 4.59, 'learning_rate': 3.626666666666667e-05, 'epoch': 0.01}
^^^^^ losses:  4.617856502532959 4.153012752532959 0.04638671875
{'loss': 4.6179, 'learning_rate': 3.68e-05, 'epoch': 0.01}
^^^^^ losses:  4.673698902130127 4.251823902130127 0.042236328125
{'loss': 4.6737, 'learning_rate': 3.733333333333334e-05, 'epoch': 0.01}
^^^^^ losses:  4.5836567878723145 4.1461567878723145 0.043701171875
{'loss': 4.5837, 'learning_rate': 3.786666666666667e-05, 'epoch': 0.01}
^^^^^ losses:  4.587129592895508 4.161348342895508 0.04248046875
{'loss': 4.5871, 'learning_rate': 3.8400000000000005e-05, 'epoch': 0.01}
^^^^^ losses:  4.45872688293457 4.06614875793457 0.039306640625
{'loss': 4.4587, 'learning_rate': 3.8933333333333336e-05, 'epoch': 0.01}
^^^^^ losses:  4.4090776443481445 4.0106401443481445 0.039794921875
{'loss': 4.4091, 'learning_rate': 3.9466666666666666e-05, 'epoch': 0.01}
^^^^^ losses:  4.230222225189209 3.857175350189209 0.037353515625
{'loss': 4.2302, 'learning_rate': 4e-05, 'epoch': 0.01}
^^^^^ losses:  4.460837364196777 4.128806114196777 0.033203125
{'loss': 4.4608, 'learning_rate': 4.0533333333333334e-05, 'epoch': 0.01}
^^^^^ losses:  4.494933128356934 4.160948753356934 0.033447265625
{'loss': 4.4949, 'learning_rate': 4.106666666666667e-05, 'epoch': 0.01}
^^^^^ losses:  4.630772113800049 4.316318988800049 0.031494140625
{'loss': 4.6308, 'learning_rate': 4.16e-05, 'epoch': 0.01}
^^^^^ losses:  4.410383224487305 4.097883224487305 0.03125
{'loss': 4.4104, 'learning_rate': 4.213333333333334e-05, 'epoch': 0.01}
^^^^^ losses:  4.431458473205566 4.122864723205566 0.03076171875
{'loss': 4.4315, 'learning_rate': 4.266666666666667e-05, 'epoch': 0.01}
save models to ./checkpoints/noneedtrain_depth_annealing-llava-v1.5-7b-task-lora/checkpoint-80
Non-default generation parameters: {'max_length': 4096}
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
^^^^^ losses:  4.257635116577148 3.9705259799957275 0.0286865234375
  1%|▍                                                    | 96/12500 [06:55<14:30:27,  4.21s/it]
{'loss': 4.2576, 'learning_rate': 4.32e-05, 'epoch': 0.01}
^^^^^ losses:  4.325612545013428 4.028737545013428 0.02978515625
{'loss': 4.3256, 'learning_rate': 4.373333333333334e-05, 'epoch': 0.01}
^^^^^ losses:  4.250548362731934 3.9946887493133545 0.025634765625
{'loss': 4.2505, 'learning_rate': 4.426666666666667e-05, 'epoch': 0.01}
^^^^^ losses:  4.3948235511779785 4.1038079261779785 0.029052734375
{'loss': 4.3948, 'learning_rate': 4.4800000000000005e-05, 'epoch': 0.01}
^^^^^ losses:  4.343317985534668 4.065974235534668 0.0277099609375
{'loss': 4.3433, 'learning_rate': 4.5333333333333335e-05, 'epoch': 0.01}
^^^^^ losses:  4.259969711303711 3.95723557472229 0.0302734375
{'loss': 4.26, 'learning_rate': 4.5866666666666666e-05, 'epoch': 0.01}
^^^^^ losses:  4.37197732925415 4.11807107925415 0.025390625
{'loss': 4.372, 'learning_rate': 4.64e-05, 'epoch': 0.01}
^^^^^ losses:  4.435723304748535 4.190606117248535 0.0245361328125
{'loss': 4.4357, 'learning_rate': 4.6933333333333333e-05, 'epoch': 0.01}
^^^^^ losses:  4.3682169914245605 4.1230998039245605 0.0245361328125
{'loss': 4.3682, 'learning_rate': 4.746666666666667e-05, 'epoch': 0.01}
^^^^^ losses:  4.303964138031006 4.030526638031006 0.02734375
{'loss': 4.304, 'learning_rate': 4.8e-05, 'epoch': 0.01}
^^^^^ losses:  4.220757007598877 4.006889820098877 0.0213623046875
{'loss': 4.2208, 'learning_rate': 4.853333333333334e-05, 'epoch': 0.01}
^^^^^ losses:  4.272689342498779 4.047103404998779 0.0225830078125
{'loss': 4.2727, 'learning_rate': 4.906666666666667e-05, 'epoch': 0.01}
^^^^^ losses:  4.328641414642334 4.074735164642334 0.025390625
{'loss': 4.3286, 'learning_rate': 4.96e-05, 'epoch': 0.01}
^^^^^ losses:  4.2568745613098145 4.0430073738098145 0.0213623046875
{'loss': 4.2569, 'learning_rate': 5.013333333333333e-05, 'epoch': 0.01}
^^^^^ losses:  4.158851623535156 3.9264297485351562 0.023193359375
{'loss': 4.1589, 'learning_rate': 5.0666666666666674e-05, 'epoch': 0.01}
^^^^^ losses:  4.37733268737793 4.13319206237793 0.0244140625
{'loss': 4.3773, 'learning_rate': 5.1200000000000004e-05, 'epoch': 0.01}
^^^^^ losses:  4.320145606994629 4.099442481994629 0.0220947265625
{'loss': 4.3201, 'learning_rate': 5.1733333333333335e-05, 'epoch': 0.01}
^^^^^ losses:  4.231072425842285 3.9888851642608643 0.024169921875
{'loss': 4.2311, 'learning_rate': 5.2266666666666665e-05, 'epoch': 0.01}
^^^^^ losses:  4.221763610839844 3.972740411758423 0.02490234375
{'loss': 4.2218, 'learning_rate': 5.28e-05, 'epoch': 0.01}
^^^^^ losses:  4.492554187774658 4.275757312774658 0.021728515625
{'loss': 4.4926, 'learning_rate': 5.333333333333333e-05, 'epoch': 0.01}
save models to ./checkpoints/noneedtrain_depth_annealing-llava-v1.5-7b-task-lora/checkpoint-100
Non-default generation parameters: {'max_length': 4096}
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
^^^^^ losses:  4.163202285766602 3.9298040866851807 0.0233154296875
  1%|▍                                                   | 109/12500 [07:51<14:35:33,  4.24s/it]
{'loss': 4.1632, 'learning_rate': 5.3866666666666664e-05, 'epoch': 0.01}
^^^^^ losses:  4.088519096374512 3.8502378463745117 0.0238037109375
{'loss': 4.0885, 'learning_rate': 5.440000000000001e-05, 'epoch': 0.01}
^^^^^ losses:  4.205977439880371 3.979414701461792 0.022705078125
{'loss': 4.206, 'learning_rate': 5.493333333333334e-05, 'epoch': 0.01}
^^^^^ losses:  4.195858001708984 3.9712483882904053 0.0224609375
{'loss': 4.1959, 'learning_rate': 5.546666666666667e-05, 'epoch': 0.01}
^^^^^ losses:  4.193197250366211 3.983236074447632 0.02099609375
{'loss': 4.1932, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.01}
^^^^^ losses:  4.043587684631348 3.8287441730499268 0.021484375
{'loss': 4.0436, 'learning_rate': 5.6533333333333336e-05, 'epoch': 0.01}
^^^^^ losses:  4.0831451416015625 3.8448638916015625 0.0238037109375
{'loss': 4.0831, 'learning_rate': 5.706666666666667e-05, 'epoch': 0.01}
^^^^^ losses:  4.087273597717285 3.8724300861358643 0.021484375
{'loss': 4.0873, 'learning_rate': 5.76e-05, 'epoch': 0.01}
^^^^^ losses:  4.121510028839111 3.9066662788391113 0.021484375
{'loss': 4.1215, 'learning_rate': 5.813333333333334e-05, 'epoch': 0.01}
^^^^^ losses:  4.210721492767334 3.974393367767334 0.023681640625
